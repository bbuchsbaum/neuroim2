Critical Notes and Cheatsheet for Implementing the Three HDF5 Data Specifications in R using hdf5r

This guide provides essential information for implementing the LabeledVolumeSet, ClusteredTimeSeries, and LatentNeuroVectors data specifications in R using the hdf5r package. It focuses on critical aspects of the hdf5r API, data handling, and compression features relevant to these implementations.

General hdf5r Usage Overview

	•	Creating HDF5 Files: Use H5File$new(filename, mode) to create or open an HDF5 file.
	•	Creating Groups: Use file$create_group(group_name) to create a group within the file.
	•	Creating Datasets: Use group$create_dataset(name, data, dtype, space, chunk_dims, compression, fill_value) to create a dataset.
	•	Attributes: Use h5attr(dataset, "attribute_name") <- value to assign attributes to datasets or groups.
	•	Data Types: Define data types using built-in h5types or create custom types.
	•	Compression: Specify compression using the dataset_create_pl property list when creating datasets.

Implementation Details for Each Specification

1. LabeledVolumeSet

Objective: Store 3D volumes with labels and metadata, allowing compression at the volume level.

Key Components:

	•	Header Group: Contains NIfTI-1 header fields as attributes.
	•	Volumes Group: Stores individual 3D volumes as datasets.
	•	Labels: Volume labels stored as attributes or separate datasets.
	•	Compression: Apply compression to individual volume datasets.

Implementation Steps:

	1.	Create the HDF5 File and Root Group:

library(hdf5r)
file <- H5File$new("labeled_volume_set.h5", mode = "w")


	2.	Create Header Group and Assign Attributes:

header_group <- file$create_group("header")
# Assign NIfTI-1 header fields as attributes
h5attr(header_group, "sizeof_hdr") <- 348L
h5attr(header_group, "dim") <- c(4L, dim_x, dim_y, dim_z, 1L, 1L, 1L, 1L)
# Continue for other header fields...


	3.	Create Volumes Group:

volumes_group <- file$create_group("volumes")


	4.	Store Volumes with Compression:

volume_data <- array(data, dim = c(dim_x, dim_y, dim_z))
dataset_create_pl <- H5P_DATASET_CREATE$new()
dataset_create_pl$set_chunk(chunk_dims)$set_deflate(compression_level)
volumes_group$create_dataset(
  name = "volume_1",
  data = volume_data,
  dtype = h5types$H5T_IEEE_F32LE,
  dataset_create_pl = dataset_create_pl
)


	5.	Assign Labels and Metadata:

h5attr(volumes_group[["volume_1"]], "label") <- "Anatomical Scan"
h5attr(volumes_group[["volume_1"]], "description") <- "T1-weighted MRI"


	6.	Close the File:

file$close_all()



Critical API Usage:

	•	Dataset Creation with Compression:
	•	Use H5P_DATASET_CREATE$new() to create a property list.
	•	Set chunking with $set_chunk(chunk_dims).
	•	Apply compression with $set_deflate(level).
	•	Assigning Attributes:
	•	Use h5attr(dataset, "attribute_name") <- value.

2. ClusteredTimeSeries

Objective: Store time-series data grouped into clusters for efficient storage and access.

Key Components:

	•	Header Group: Contains NIfTI-1 header fields.
	•	Mask Dataset: Binary mask indicating non-zero voxels.
	•	Cluster Map Dataset: Maps each voxel to a cluster ID.
	•	Voxel Coordinates Dataset: Coordinates of non-zero voxels.
	•	Clusters Group: Contains metadata for each cluster.
	•	Scans Group: Contains time-series data for each scan, organized by clusters.

Implementation Steps:

	1.	Create the HDF5 File and Root Group:

file <- H5File$new("clustered_time_series.h5", mode = "w")


	2.	Create Header Group and Assign Attributes:

header_group <- file$create_group("header")
# Assign NIfTI-1 header fields as attributes...


	3.	Create Mask Dataset:

mask_data <- array(mask, dim = c(dim_x, dim_y, dim_z))
file$create_dataset("mask", data = mask_data, dtype = h5types$H5T_STD_U8LE)


	4.	Create Cluster Map Dataset:

cluster_map_data <- cluster_ids  # Vector of cluster IDs per voxel
file$create_dataset("cluster_map", data = cluster_map_data, dtype = h5types$H5T_STD_I32LE)


	5.	Create Voxel Coordinates Dataset:

voxel_coords_data <- matrix(c(x_coords, y_coords, z_coords), ncol = 3)
file$create_dataset("voxel_coords", data = voxel_coords_data, dtype = h5types$H5T_STD_I32LE)


	6.	Create Clusters Group and Metadata:

clusters_group <- file$create_group("clusters")
clusters_group$create_dataset("cluster_ids", data = unique_cluster_ids, dtype = h5types$H5T_STD_I32LE)
# Optionally, create a compound dataset for cluster metadata


	7.	Create Scans Group and Store Time-Series Data:

scans_group <- file$create_group("scans")
scan_name <- "run1"
scan_group <- scans_group$create_group(scan_name)
# Assign scan metadata
h5attr(scan_group, "subject_id") <- "subj01"
# For each cluster, create a dataset
clusters_in_scan <- unique(cluster_ids)
for (cluster_id in clusters_in_scan) {
  cluster_data <- time_series_data[[cluster_id]]  # Matrix of [voxels_in_cluster, timepoints]
  dataset_create_pl <- H5P_DATASET_CREATE$new()
  dataset_create_pl$set_chunk(chunk_dims)$set_deflate(compression_level)
  scan_group$create_dataset(
    name = paste0("cluster_", cluster_id),
    data = cluster_data,
    dtype = h5types$H5T_IEEE_F32LE,
    dataset_create_pl = dataset_create_pl
  )
}


	8.	Close the File:

file$close_all()



Critical API Usage:

	•	Handling Compound Data Types:
	•	Use H5T_COMPOUND$new() to create compound data types for cluster metadata if needed.
	•	Dataset Creation with Compression:
	•	Similar to the previous spec, use property lists to set chunking and compression.
	•	Data Alignment:
	•	Ensure that the order of voxels in voxel_coords aligns with the data in cluster datasets.

3. LatentNeuroVectors

Objective: Represent neuroimaging data in a latent space using embeddings, storing basis vectors and loadings.

Key Components:

	•	Header Group: Contains NIfTI-1 header fields.
	•	Mask Dataset: Binary mask indicating non-zero voxels.
	•	Voxel Coordinates Dataset: Coordinates of non-zero voxels.
	•	Basis Group: Stores the basis set or references it.
	•	Offset Dataset: Optional offset vector for data centering.
	•	Scans Group: Contains embedding vectors (loadings) for each scan.

Implementation Steps:

	1.	Create the HDF5 File and Root Group:

file <- H5File$new("latent_neuro_vectors.h5", mode = "w")


	2.	Create Header Group and Assign Attributes:

header_group <- file$create_group("header")
# Assign NIfTI-1 header fields as attributes...


	3.	Create Mask Dataset:

mask_data <- array(mask, dim = c(dim_x, dim_y, dim_z))
file$create_dataset("mask", data = mask_data, dtype = h5types$H5T_STD_U8LE)


	4.	Create Voxel Coordinates Dataset:

voxel_coords_data <- matrix(c(x_coords, y_coords, z_coords), ncol = 3)
file$create_dataset("voxel_coords", data = voxel_coords_data, dtype = h5types$H5T_STD_I32LE)


	5.	Create Basis Group:
	•	Option 1: Store Basis Matrix Directly:

basis_data <- basis_matrix  # Matrix of [k, number_of_nonzero_voxels]
basis_group <- file$create_group("basis")
basis_group$create_dataset("basis_matrix", data = basis_data, dtype = h5types$H5T_IEEE_F32LE)


	•	Option 2: Reference Basis by Identifier:

basis_group <- file$create_group("basis")
h5attr(basis_group, "basis_id") <- "DCT_Basis_v1"
h5attr(basis_group, "basis_type") <- "DCT"
h5attr(basis_group, "basis_parameters") <- list(param1 = value1, param2 = value2)


	6.	Create Offset Dataset (Optional):

offset_data <- offset_vector  # Vector of length number_of_nonzero_voxels
file$create_dataset("offset", data = offset_data, dtype = h5types$H5T_IEEE_F32LE)


	7.	Create Scans Group and Store Embeddings:

scans_group <- file$create_group("scans")
scan_name <- "run1"
scan_group <- scans_group$create_group(scan_name)
# Assign scan metadata
h5attr(scan_group, "subject_id") <- "subj01"
# Store embedding vectors
embedding_data <- embedding_matrix  # Matrix of [run_length, k]
dataset_create_pl <- H5P_DATASET_CREATE$new()
dataset_create_pl$set_chunk(chunk_dims)$set_deflate(compression_level)
scan_group$create_dataset(
  name = "embedding",
  data = embedding_data,
  dtype = h5types$H5T_IEEE_F32LE,
  dataset_create_pl = dataset_create_pl
)


	8.	Close the File:

file$close_all()



Critical API Usage:

	•	Handling Variable-Length Data:
	•	If using variable-length data types, ensure correct handling with H5T_VLEN$new().
	•	Dataset Creation with Compression:
	•	Apply compression to embedding datasets to optimize storage.
	•	Attributes for Basis Reference:
	•	Use attributes to store basis_id, basis_type, and basis_parameters when referencing the basis.

Critical Aspects of hdf5r for Implementation

Data Types and Conversions:

	•	Standard Data Types: Use h5types for predefined HDF5 data types (e.g., h5types$H5T_IEEE_F32LE for 32-bit floats).
	•	Custom Data Types: Create custom data types using classes like H5T_ENUM, H5T_COMPOUND, H5T_ARRAY, and H5T_STRING.
	•	Compound Data Types: Use for datasets that require multiple fields (e.g., cluster metadata).

Compression and Chunking:

	•	Compression:
	•	Use Gzip compression via $set_deflate(level) on the dataset creation property list (H5P_DATASET_CREATE).
	•	Level ranges from 0 (no compression) to 9 (maximum compression).
	•	Chunking:
	•	Necessary for compression and extendable datasets.
	•	Set chunk dimensions with $set_chunk(chunk_dims).

Handling Attributes:

	•	Assigning Attributes:
	•	Use h5attr(object, "attribute_name") <- value to assign attributes to datasets or groups.
	•	Useful for storing metadata like labels, descriptions, or parameters.

Managing HDF5 Resources:

	•	Closing Files and Objects:
	•	Use file$close_all() to close the file and all associated objects.
	•	Important to prevent file locking issues.
	•	Automatic Resource Management:
	•	hdf5r uses R’s garbage collector to manage HDF5 IDs.
	•	Be cautious with open objects to avoid resource leaks.

Advanced Features:

	•	Dataset Extensions:
	•	Define maxdims in space to allow dataset resizing.
	•	Use dataset$set_extent(new_dims) to resize datasets if needed.
	•	Variable-Length Data Types:
	•	Use H5T_VLEN$new(dtype_base) for datasets with variable-length entries (e.g., lists).
	•	Enumerated Types:
	•	Use H5T_ENUM$new(labels, values) to define datasets with enumerated types (e.g., cluster IDs).

Tips for Efficient Implementation

	•	Consistent Voxel Ordering:
	•	Ensure the order of voxels in voxel_coords matches across datasets for alignment.
	•	Compression Trade-offs:
	•	Higher compression levels reduce file size but may impact read/write performance.
	•	Test different levels to find an optimal balance.
	•	Chunk Size Selection:
	•	Choose chunk sizes that match typical data access patterns to optimize performance.
	•	For time-series data, chunk along the time dimension if accessing data across time points.
	•	Attribute Usage:
	•	Store essential metadata as attributes for quick access.
	•	Use consistent naming conventions for attributes.
	•	Data Validation:
	•	Implement checks to ensure data dimensions and types match the specifications.
	•	Use assertions or validation functions during data writing.

Example: Creating a Compressed Dataset with hdf5r

library(hdf5r)

# Create HDF5 file
file <- H5File$new("example.h5", mode = "w")

# Create dataset creation property list with compression
dcpl <- H5P_DATASET_CREATE$new()
dcpl$set_chunk(c(100, 100))$set_deflate(9)

# Create dataset with compression
data_matrix <- matrix(runif(10000), nrow = 100, ncol = 100)
file$create_dataset(
  name = "compressed_dataset",
  data = data_matrix,
  dtype = h5types$H5T_IEEE_F32LE,
  dataset_create_pl = dcpl
)

# Close the file
file$close_all()

References

	•	hdf5r Package Documentation: Provides comprehensive details on the API and advanced features.
	•	HDF5 User’s Guide: Offers in-depth information on the HDF5 data model and file format.

By focusing on these critical aspects and utilizing the hdf5r package effectively, the implementation of the three data specifications in R can be accomplished efficiently and accurately.