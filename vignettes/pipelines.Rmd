---
title: "Pipelines"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Pipelines}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
library(neuroim2)
```

Pipelining operations using a functional approach
===================

The `neuroim2` packages provides a set of functions that allows one to split image data in various ways to facilitate algorithms that work separately on pieces of data. By breaking a dataset up into parts, we can also more easily parallelize certain operations. 

## Splitting an image into connected components

First we load in an example volume, assign it random values, and find its connected components with a threshold of .9

```{r}
  
      file_name <- system.file("extdata", "global_mask.nii", package="neuroim2")
      vol <- read_vol(file_name)
      mask.idx <- which(vol>0)
      vol[mask.idx] <- runif(length(mask.idx))
      comp <- conn_comp(vol, threshold=.9)
```

Now we want to find the average value in each of the connected components using the `split_clusters` function. Since `conn_comp` returns a `ClusteredNeuroVol` containing the cluster indices, we use that to split the original volume into a list of `ROIVol`s and compute the mean over each one.

```{r}

mvals <- vol %>% split_clusters(comp$index) %>% map_dbl( ~ mean(.))

```
    
Suppose we want to compute the local standard deviation within a 4mm radius of each voxel. We can use the `searchlight` function to construct a list of spherical ROIs centered on every voxel in the input set.

```{r}

sdvol <- vol %>% searchlight(radius=5, eager=TRUE) %>% map_dbl( ~ sd(.)) %>% NeuroVol(space=space(vol), indices=which(vol!=0))

```
